import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sn

from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LinearRegression,SGDRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor


from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score
import joblib

df=pd.read_csv('solarpower.csv')

df.head()

df.tail()

df.info()

df.shape

df.columns

df.isnull().sum()

df[df["average-wind-speed-(period)"].isnull()]


median_value = df["average-wind-speed-(period)"].median()
df["average-wind-speed-(period)"] = df["average-wind-speed-(period)"].fillna(median_value)


df.isnull().sum()

df.duplicated().sum()

df.describe()

#target distrubution

sn.histplot(df["power-generated"],kde=True)
plt.title('distrubution of power generated')
plt.xlabel('power generated')
plt.ylabel('frequency')
plt.show()

df.hist(figsize=(10,15))
plt.show()

#outlier detection

for column in df.columns:
    plt.figure(figsize=(12,6))
    sn.boxplot(data=df[column])
    plt.xticks(rotation=90)
    plt.title("Boxplot for outlier detection")
    plt.show()



#correlation
plt.figure(figsize=(10,6))
sn.heatmap(df.corr(), annot=True, cmap="coolwarm")
plt.show()


sn.pairplot(df,diag_kind='kde')
plt.show()


#feature and target split

x=df.drop('power-generated',axis=1)
y=df['power-generated']

#train-test split

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

#scaling
scaler=StandardScaler()
x_train_scaled=scaler.fit_transform(x_train)
x_test_scaled=scaler.transform(x_test)

lr=LinearRegression()
lr.fit(x_train_scaled,y_train)
y_hat=lr.predict(x_test_scaled)

lr_rmse=np.sqrt(mean_squared_error(y_test, y_hat))
lr_mae=mean_absolute_error(y_test, y_hat)
lr_r2=r2_score(y_test, y_hat)

print('lr_rmse',lr_rmse)
print('lr_mae',lr_mae)
print('lr_r2',lr_r2)

plt.figure(figsize=(8,6))
plt.scatter(y_test, y_hat)
plt.xlabel("Actual Power Generated")
plt.ylabel("Predicted Power Generated")
plt.title("Actual vs Predicted Values")
plt.show()

sgd = SGDRegressor(max_iter=1000, tol=1e-3)
sgd.fit(x_train_scaled, y_train)
y_hat= sgd.predict(x_test_scaled)

sgd_rmse = np.sqrt(mean_squared_error(y_test, y_hat))
sgd_mae=mean_absolute_error(y_test, y_hat)
sgd_r2 = r2_score(y_test, y_hat)

print('sgd_rmse',sgd_rmse)
print('sgd_mae',sgd_mae)
print('sgd_r2',sgd_r2)

plt.figure(figsize=(8,6))
plt.scatter(y_test, y_hat)
plt.xlabel("Actual Power Generated")
plt.ylabel("Predicted Power Generated")
plt.title("Actual vs Predicted Values")
plt.show()

dt = DecisionTreeRegressor(random_state=42)
dt.fit(x_train, y_train)
y_hat = dt.predict(x_test)

dt_rmse = np.sqrt(mean_squared_error(y_test, y_hat))
dt_mae=mean_absolute_error(y_test,y_hat)
dt_r2 = r2_score(y_test, y_hat)

print('dt_rmse',dt_rmse)
print('dt_mae',dt_mae)
print('dt_r2',dt_r2)

plt.figure(figsize=(8,6))
plt.scatter(y_test, y_hat)
plt.xlabel("Actual Power Generated")
plt.ylabel("Predicted Power Generated")
plt.title("Actual vs Predicted Values")
plt.show()

rf=RandomForestRegressor(n_estimators=100,random_state=42)
rf.fit(x_train,y_train)
y_hat=rf.predict(x_test)

rf_rmse=np.sqrt(mean_squared_error(y_test, y_hat))
rf_mae=mean_absolute_error(y_test, y_hat)
rf_r2=r2_score(y_test, y_hat)

print('rf_rmse',rf_rmse)
print('rf_mae',rf_mae)
print('rf_r2',rf_r2)

plt.figure(figsize=(8,6))
plt.scatter(y_test, y_hat)
plt.xlabel("Actual Power Generated")
plt.ylabel("Predicted Power Generated")
plt.title("Actual vs Predicted Values")
plt.show()

feature_importance = pd.DataFrame({
    "Feature": x.columns,
    "Importance": rf.feature_importances_
}).sort_values(by="Importance", ascending=False)

print("\nFEATURE IMPORTANCE")
print(feature_importance)

plt.figure(figsize=(6,4))
plt.bar(feature_importance['Feature'], feature_importance['Importance'])
plt.xlabel('Importance')
plt.ylabel('feature')
plt.xticks(rotation=90)
plt.title("Feature Importance - Random Forest")
plt.show()


compare= pd.DataFrame({
    "Model": ['Linear Regression','Gradient Descent','Decison Tree','Random Forest'],
    "R2 Score": [lr_r2, sgd_r2, dt_r2, rf_r2],
    "MAE": [lr_mae, sgd_mae, dt_mae, rf_mae],
    "RMSE": [lr_rmse, sgd_rmse, dt_rmse, rf_rmse]
}).sort_values(by='R2 Score', ascending=False)

print("\nMODEL COMPARISON")
print(compare)

plt.figure(figsize=(8,4))
plt.bar(compare['Model'], compare['R2 Score'])
plt.xlabel('Models')
plt.ylabel('R2 Score')
plt.title('Model Comparison')
plt.show()

best=compare.iloc[0]
print('BEST MODEL:', best['Model'])

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

rf_tuned = RandomForestRegressor(random_state=42)

grid = GridSearchCV(
    rf_tuned,
    param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1
)

grid.fit(x_train, y_train)

best_rf = grid.best_estimator_

rf_tuned_pred = best_rf.predict(x_test)

rf_tuned_mae = mean_absolute_error(y_test, rf_tuned_pred)
rf_tuned_rmse = np.sqrt(mean_squared_error(y_test, rf_tuned_pred))
rf_tuned_r2 = r2_score(y_test, rf_tuned_pred)

print('\nTUNED RANDOM FOREST PERFORMANCE:')
print('MAE:', rf_tuned_mae)
print('RMSE:', rf_tuned_rmse)
print('R2 Score:', rf_tuned_r2)
print('Best Parameters:', grid.best_params_)


joblib.dump(best_rf, 'best_rf.joblib')
print('model saved')
